\documentclass[conference]{IEEEtran}

\usepackage[british]{babel}
\usepackage[hyphens]{url}
\usepackage[pdftex,colorlinks=true]{hyperref}

\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}

\title{Open Benchmarking: A Model for Research Software}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Tom Crick}
\IEEEauthorblockA{Department of Computing\\
Cardiff Metropolitan University\\
Cardiff, UK\\
Email: {\url{tcrick@cardiffmet.ac.uk}}}
\and
\IEEEauthorblockN{Benjamin A. Hall}
\IEEEauthorblockA{Microsoft Research\\
Cambridge, UK\\
Email: {\url{benhall@microsoft.com}}}
\and
\IEEEauthorblockN{Samin Ishtiaq}
\IEEEauthorblockA{Microsoft Research\\
Cambridge, UK\\
Email: {\url{samin.ishtiaq@microsoft.com}}} }

\maketitle

\begin{abstract}
Abstract here...
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}

Marc Andreessen famously said a few years ago that software is eating
the world. It's true: we live in a computational world, dominated by
software. Our bank cards, our keys, our bookstores, our shopping, our
cars \dots are all being replaced by software.

This is true in science too. New results, benchmarks, even proofs
cannot be done without software. And not much of this software is
throw-away scripts, so it needs to be maintainable and reuseable. Even
more so for science, where reproducibility is the basis of the
methodology.

Sadly, if we want to be truthful about this, then the scientific
literature related to software tools (CAV Tool papers, Nature Tools
and Method(?)  papers, etc) are not doing science themselves. How many
of them are reproducible? How many explain their experimental
methodologies, in particular the basis for their benchmarking?

There are reasons why the state is like this. There are many
non-technical impediments to making software maintainable and
re-useable. The pressure to ``make the discovery'' and publish quickly
dis-incentivizes careful software development. And releasing code
might give your competitors an advantage.

% let's all give an example of where we haven't done this ourselves?
% would show it's easy just to publish a paper and forget about the
% code/benchmarking...
The authors are not immune to this problem~\cite{crick-et-al:2009}...

Things can be much better. And we have seen the rise of initiatives,
such as the Software
Carpentry~\footnote{\url{http://software-carpentry.org/}}, Software
Sustainability Institute~\footnote{\url{http://www.software.ac.uk/}}
and the UK Community of Research Software
Engineers~\footnote{\url{http://www.rse.ac.uk}} to cultivate
world-class research through software, as well as develop software
skills and raise the profile of research software engineers.



Big focus on changing publishing and academic dissemination
e.g. \cite{stodden-et-al:2013,fursin+dubach:2014}.

Recommendations on where to publish software:
http://www.software.ac.uk/resources/guides/which-journals-should-i-publish-my-software


In this short paper, we present a small set of recommendations which
we hope will lead to better, more sustainable, more re-useable
software. The basis for many of these recommendations is the basic
scientific tenet of open-ness.

This is not a new problem; there has been previous work in this
area...~\cite{sim-et-al:2003,chirigati-et-al:2013}, as well as http://ctuning.org/...

Cite~\cite{collberg-et-al:2014}


\section{Can I implement your algorithm?}
The reader cannot re-implement the paper's algorithm. Reproducibility
is a basic tenet of good Science. Many algorithms described in premier
scientific/adademic conferences cannot be re-implemented, often
because because the description lacks sufficient detail and reasoing
about why something was done one way or another.

Recommendation: the paper must describe the algorithm in such a way
that it's implementable by a third party. We recommend that premier
scientific conferences have a special track for papers that only
re-implement past papers' algorithms/techniques/tools.


\section{Be a better person}

scientist education~\cite{Wilson2014}. Even basic training can improve
the habits of junior programmers, by introducing the concepts of
revision control, unit tests etc

some scientists may not have had any formal/informal training in programming, and even a basic introduction may improve the software. Individuals leading the software carpentry workshop reported a large improvement in awareness and skills following the course \url{http://philipwfowler.wordpress.com/2013/12/19/the-oxford-software-carpentry-boot-camp-one-year-on/}


\section{Latin mass is better}

use of high level language, with type checking (preferably units of
measure). Agressive type checking avoids a subset of bugs which can
arise due to incorrectly written functions e.g. well publicised NASA
problems with a Mars orbiter
(\url{http://www.cnn.com/TECH/space/9909/30/mars.metric.02/}). Problems
found using in house software for crystallography lead to 5
retractions \cite{Miller2006}, that arose due to a bug which inverted
the phases. A pressure coupling bug in GROMACS~\cite{Hess2008},
\url{http://redmine.gromacs.org/issues/14}, arose due to the
inappropriate swapping of a pressure term with a stress tensor.

Readability of SSReflect vs standard Coq. Indents, vernacular, repeatabilty of proof checking because parameters are named rather than un-named. 
cite Gonthier's Ad-hoc Proof Automation paper. 

\section{Set the code free} 

software development is hard, but sharing and using others code is
relatively easy. Code developing groups may wish to hold onto their
code as a competitive advantage, especially if there exist larger
competitors who could use the available code to ``reverse scoop'' and
lead into a promising new area opened by someone else.

the code should have an appropriate open source license
(recommendations?). A wide variety of licenses exist for molecular
dynamics software, with different degrees of openness (GROMACS=LGPL~\cite{Hess2008}
,CHARMM/CHARMm and Desmond=Academic/Commercial software licences~\cite{Brooks2009,Bowers2006}, Amber
and NAMD=custom open-like licence).

Whilst the lack of an open licence is not necessarily a barrier to
widespread adoption (e.g. GAUSSIAN), it facilitates testing and
comparison, and promotes competition.

Z3 is another good example. The code is not open source, but providing
the code under an academic license is much better than not providing
the code at all.

licences may prohibit viewing the source, modifying, sharing, or even
analysing software performance and behaviour e.g. GAUSSIAN~\cite{Giles2004}

the code should be available on github or similar source control/repo?

\section{Test it to see}
link from shared code... shared code is more test-able. 

unshared code is untestable.  testing new scientific software is
difficult, as until the software is complete unit tests may not be
available

some models may be chaotic and influenced by floating point errors
(e.g. molecular dynamics), further frustrating testing. Example:
Sidekick is an automated tool for building molecular models and
performing simualations~\cite{Hall2014Sidekick}. Each system is
simulated from an different initial random seed, and under most
circumstances this is the only difference expected between
replicas. However, on a mixed cluster with AMD and intel nodes, the
difference in architecture was found to alter the number of water
molecules added to each system by 1. This meant that the same
simulation performed on different architectures would
diverge. Similarly, in a different simulation engine, different
neighbour searching strategies gave divergent simulations due to the
differing order in which forces were summed.



\section{Lineage} 

the code should include links to papers publishing individual
algorithms and the code should include explicit relationships to other
projects on the repo (i.e. B was branched from A). This ensure that
the researchers and software developers working upstream of the
current project are properly credited, encouraging future sharing

main proposal: automatic online dependency/build/traffic light system
with github hooks?

more main proposal: a lineage of the software (history/branches) and a
(more limited) lineage of the algorithm its built on

\section{YMMV}

\begin{quote}
You can download our code from the URL supplied. Good luck downloading the only postdoc who can get it to run, though \# overlyhonestmethods

\url{https://twitter.com/ianholmes/status/288689712636493824} 
\end{quote}


probably murdered before-hand. 

The tool that the paper describes either doesn't exist for
download. Or runs only one one particular platform. Or might run for
the author, for a while, but will bot-rot so soon that even the author
cannot compile it in two months time.



you should provide details of \emph{how} you built and wrote the software. All the below features can influence the behaviour of your software

you should provide the compiler and build toolchain

you should provide builds tools/makefiles/ant/etc and build instructions

you should list/link to all non-standard packages/libraries/APIs

you should note the hardware and OS used


\section{Form over function}

develop standard file formats which appropriately constrain the
models. Example: qualitative networks and Boolean networks are
standard types of biological formal
model~\cite{Kauffman1969,Schaub2007}. They can be expressed in SMV,
but this means that standard behaviours have to be hard coded for each
variable, introducing the possibility for errors. In the
BioModelAnalyzer~\cite{Benque2012}, the XML contains \emph{only} the
modifiable parameters limiting the possibility for error.


\section{9.63s} 

The benchmarks the tool describes are fashioned only for this instance
of this time. They might claim to be from the Windows device driver
set, but the reality is that they are are stripped down versions of
the originals. Stripped down so much as to be useless to anyone but
the author vs the referee. It's worst than that really: enough
benchmarks are included to beat other tools. The comparisons are never
fair. (Neither are other peoples' comparisons against your tool.) If
every paper has to be novel, then every benchmark, too, will be novel;
there is no historical truth in new, crafted benchmarks.


Benchmarks should be public, they should be heavily curated (every
test/assertion should be justified). Papers should be penalized if
they don't use these public benchmarks.

you should include the models used for testing/benchmarking, or
(better) deposit the models in a standard format, in a standard
repository e.g. the protein data bank (\url{http://www.pdb.org}) and
Systems Biology Markup Language~\cite{Hucka2003,Chaouiya2013}. This
would allow the models to be taken and easily analysed in alternative
programs where available.

\section{Welcome to the Web 2.0, guys; you're 10 years late} 

you could include an interface to the tool and a model in the web
version of the paper, to demonstrate how it works i.e. make the paper
``executable''. This would allow users who are not able to install
necessary dependecies etc. to explore the model \cite{Hall2014}

Virtual machines could make testing of scaling properties more simple. 
Build a cluster of slow nodes in the cloud to demonstrate how well the
software scales for parallel calculations


\section{Conclusion}
Follow our recommendations!



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{37}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% BibTeX users
\bibliographystyle{IEEEtran}      % basic style, author-year citations
\bibliography{wssspe2}   % name your BibTeX data base

\end{document}
