\documentclass[conference]{IEEEtran}

\usepackage[british]{babel}
\usepackage[hyphens]{url}
\usepackage[pdftex,colorlinks=true]{hyperref}

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Ten(?) Top Tips for Benchmarking Research Software}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Tom Crick}
\IEEEauthorblockA{Department of Computing\\
Cardiff Metropolitan University\\
Cardiff, UK\\
Email: {\url{tcrick@cardiffmet.ac.uk}}}
\and
\IEEEauthorblockN{Benjamin A Hall}
\IEEEauthorblockA{Microsoft Research\\
Cambridge, UK\\
Email: {\url{benhall@microsoft.com}}}}
\and
\IEEEauthorblockN{Samin Ishtiaq}
\IEEEauthorblockA{Microsoft Research\\
Cambridge, UK\\
Email: {\url{samin.ishtiaq@microsoft.com}}}}

\maketitle

\begin{abstract}
Abstract here...
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}
Intro...

If we want to be truthful about it, then the Scientific Literature
about software tools (CAV Tool papers, Nature Method(?) papers, etc)
are not doing Science themselves. We're not saying all the papers are
like this, but a typical paper, if you peer beneath the shiny \LaTeX
and seemingly statistical tables of figures, is this:

\begin{enumerate}

\item The reader cannot re-implement the paper's
  algorithm. Reproducibility is a basic tenet of good Science. Many
  algorithms described in premier scientific/adademic conferences
  cannot be re-implemented, often because because the description
  lacks sufficient detail and reasoing about why something was done
  one way or another.

\item The tool that the paper describes either doesn't exist for
  download. Or runs only one one particular platform. Or might run for
  the author, for a while, but will bot-rot so soon that even the
  author cannot compile it in two months time.

\item The benchmarks the tool describes are fashioned only for this
  instance of this time. They might claim to be from the Windows
  device driver set, but the reality is that they are are stripped
  down versions of the originals. Stripped down so much as to be
  useless to anyone but the author vs the referee. It's worst than
  that really: enough benchmarks are included to beat other tools. The
  comparisons are never fair. (Neither are other peoples' comparisons
  against your tool.) If every paper has to be novel, then every
  benchmark, too, will be novel; there is no historical truth in new,
  crafted benchmarks.
\end{enumerate}

Things can be much better. 

These things present a barrier to scientific software reliability:
\begin{itemize}
\item the pressure to ``make the discovery'' and publish quickly discincentizes careful software development
\item software development is hard, but sharing and using others code is relatively easy. Code developing groups may wish to hold onto their code as a competitive advantage, especially if there exist larger competitors who could use the available code to ``reverse scoop'' and lead into a promising new area opened by someone else. unshared code is untestable\cdots
\item testing new scientific software is difficult, as until the software is complete unit tests may not be available
\item some models may be chaotic and influenced by floating point errors (e.g. molecular dynamics), further frustrating testing
\item some scientists may not have had any formal/informal training in programming, and even a basic introduction may improve the software \url{http://philipwfowler.wordpress.com/2013/12/19/the-oxford-software-carpentry-boot-camp-one-year-on/}
\end{itemize}

\section{Top Tips}
N.B. The ``rules'' should make reference to how this would improve the reusability of software that implemented the c.10 points...

Cite~\cite{collberg-et-al:2014}

\begin{itemize}
\item scientist education \url{http://f1000research.com/articles/3-62/v1}
\item the code should have an appropriate open source license (recommendations?)
\item the code should be available on github or similar source control/repo?
\item the code should include links to papers publishing individual algorithms
\item the code should include explicit relationships to other projects on the repo (i.e. B was branched from A)
\item you should provide the compiler and build toolchain
\item you should provide builds tools/makefiles/ant/etc and build instructions
\item you should list/link to all non-standard packages/libraries/APIs
\item you should note the hardware and OS used
\item you should include the models used for testing/benchmarking
\item you could include an interface to the tool and a model in the web version of the paper, to demonstrate how it works i.e. make the paper ``executable'' eg Ben's retrodiction paper 
\item main proposal: automatic online dependency/build/traffic light system with github hooks?
\item more main proposal: a lineage of the software (history/branches) and a (more limited) lineage of the algorithm its built on
\item case studies: CS, systems biology?
\end{itemize}

\section{Conclusion}
Conclusions...


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{37}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% BibTeX users
\bibliographystyle{IEEEtran}      % basic style, author-year citations
\bibliography{wssspe2}   % name your BibTeX data base

\end{document}
