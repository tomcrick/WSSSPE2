\documentclass[conference]{IEEEtran}

\usepackage[british]{babel}
\usepackage[hyphens]{url}
\usepackage[pdftex,colorlinks=true]{hyperref}

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Ten(?) Top Tips for Benchmarking Research Software}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Tom Crick}
\IEEEauthorblockA{Department of Computing\\
Cardiff Metropolitan University\\
Cardiff, UK\\
Email: {\url{tcrick@cardiffmet.ac.uk}}}
\and
\IEEEauthorblockN{Samin Ishtiaq}
\IEEEauthorblockA{Microsoft Research\\
Cambridge, UK\\
Email: {\url{samin.ishtiaq@microsoft.com}}}}

\maketitle

\begin{abstract}
Abstract here...
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}
Intro...

If we want to be truthful about it, then the Scientific Literature
about software tools (CAV Tool papers, Nature Method(?) papers, etc)
are not doing Science themselves. We're not saying all the papers are
like this, but a typical paper, if you peer beneath the shiny \LaTeX
and seemingly statistical tables of figures, is this:

\begin{enumerate}

\item The reader cannot re-implement the paper's
  algorithm. Reproducibility is a basic tenet of good Science. Many
  algorithms described in premier scientific/adademic conferences
  cannot be re-implemented, often because because the description
  lacks sufficient detail and reasoing about why something was done
  one way or another.

\item The tool that the paper describes either doesn't exist for
  download. Or runs only one one particular platform. Or might run for
  the author, for a while, but will bot-rot so soon that even the
  author cannot compile it in two months time.

\item The benchmarks the tool describes are fashioned only for this
  instance of this time. They might claim to be from the Windows
  device driver set, but the reality is that they are are stripped
  down versions of the originals. Stripped down so much as to be
  useless to anyone but the author vs the referee. It's worst than
  that really: enough benchmarks are included to beat other tools. The
  comparisons are never fair. (Neither are other peoples' comparisons
  against your tool.) If every paper has to be novel, then every
  benchmark, too, will be novel; there is no historical truth in new,
  crafted benchmarks.
\end{enumerate}

Things can be much better. 

\section{Top Tips}
N.B. The ``rules'' should make reference to how this would improve the reusability of software that implemented the c.10 points...

Cite~\cite{collberg-et-al:2014}

\begin{itemize}
\item the code should have an appropriate open source license (recommendations?)
\item the code should be available on github or similar source control/repo?
\item you should provide the compiler and build toolchain
\item you should provide builds tools/makefiles/ant/etc and build instructions
\item you should list/link to all non-standard packages/libraries/APIs
\item main proposal: automatic online dependency/build/traffic light system with github hooks?
\item case studies: CS, systems biology?
\end{itemize}

\section{Conclusion}
Conclusions...


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{37}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% BibTeX users
\bibliographystyle{IEEEtran}      % basic style, author-year citations
\bibliography{wssspe2}   % name your BibTeX data base

\end{document}